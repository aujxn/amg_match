\documentclass[a4paper,12pt]{amsart}
\input epsf
\usepackage{epsfig}
\usepackage{graphicx,xcolor}

\DeclareMathAlphabet\mathbold{OML}{cmm}{b}{it}
\textheight=8.75in
\textwidth=6.25in
\oddsidemargin=-0.1in
\evensidemargin=-0.1in
\vsize 8.2in
% Equation numbering
\numberwithin{equation}{section}
 
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{algorithm}{Algorithm}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{example}{Example}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{problem}{Problem}[section]


\newcommand{\curl}{\operatorname{curl}}
\renewcommand{\div}{\operatorname{div}}
\newcommand{\trace}{\operatorname{tr}}

\newcommand{\diag}{\operatorname{diag}}

\renewcommand{\span}{\operatorname{span}}

\def\b1{{\mathbf 1}}
\def\bv{{\mathbf v}}
\def\bu{{\mathbf u}}
\def\bw{{\mathbf w}}
\def\ba{{\mathbf a}}
\def\bb{{\mathbf b}}
\def\bc{{\mathbf c}}
\def\bg{{\mathbf g}}
\def\bh{{\mathbf h}}
\def\br{{\mathbf r}}
\def\bs{{\mathbf s}}
\def\bd{{\mathbf d}}
\def\be{{\mathbf e}}
\def\bp{{\mathbf p}}
\def\bq{{\mathbf q}}
\def\bx{{\mathbf x}}
\def\by{{\mathbf y}}
\def\bz{{\mathbf z}}
\def\bn{{\mathbf n}}
\def\bbf{{\mathbf f}}
\def\bB{{\mathbf B}}
\def\bM{{\mathbf M}}
\def\bV{{\mathbf V}}
\def\bU{{\mathbf U}}
\def\bY{{\mathbf Y}}
\def\bF{{\mathbf F}}
\def\bA{{\mathbf A}}
\def\bB{{\mathbf B}}
\def\bD{{\mathbf D}}
\def\bN{{\mathbf N}}
\def\bT{{\mathbf T}}
\def\bP{{\mathbf P}}
\def\bQ{{\mathbf Q}}
\def\bS{{\mathbf S}}
\def\bR{{\mathbf R}}
 
\def\bepsilon{{\boldsymbol \epsilon}}
\def\balpha{{\boldsymbol \alpha}}
\def\bdelta{{\boldsymbol \delta}}
\def\blambda{{\boldsymbol \lambda}}
 
 
 
%-----------------------------------------------------------------
\renewcommand{\O}{{\mathcal O}}
\newcommand{\Q}{{\mathcal Q}}
\newcommand{\R}{{\mathcal R}}
\newcommand{\A}{{\mathcal A}}
\newcommand{\B}{{\mathcal B}}
\newcommand{\C}{{\mathcal C}}
\newcommand{\D}{{\mathcal D}}
\newcommand{\F}{{\mathcal F}}
\newcommand{\G}{{\mathcal G}}
\newcommand{\I}{{\mathcal I}}
\newcommand{\J}{{\mathcal J}}
\newcommand{\M}{{\mathcal M}}
\newcommand{\N}{{\mathcal N}}
\newcommand{\X}{{\mathcal X}}
\newcommand{\Y}{{\mathcal Y}}
\newcommand{\calY}{{\mathcal Y}}
\newcommand{\calS}{{\mathcal S}}
\renewcommand{\L}{{\mathcal L}}
\renewcommand{\P}{{\mathcal P}}
 
\newcommand{\V}{\text{\bf V}}
\newcommand{\K}{{\mathcal K}}
\newcommand{\T}{{\mathcal T}}
\newcommand{\E}{{\mathcal E}}

\newcommand{\hatcalK}{\widehat{\mathcal K}}
\newcommand{\hatcalS}{\widehat{\mathcal S}}
\newcommand{\hatA}{\widehat{A}}
%% \newcommand{\grad}{\nabla}
\renewcommand{\div}{\text{div}}
\renewcommand{\curl}{\text{curl}}
%
%
\def\XVec#1{{\mathbf #1}}
\def\XNorm#1{\left\| #1 \right\|}                       % norm
\def\XIProd#1#2{\left\langle #1 ,~ #2 \right\rangle}    % inner product
 
\def\XM{\mu}
 
\def\XQ{Q}                     % interpolant
\def\Xq#1{\XVec{q}_{#1}}       % rows of Q (interpolation to point #1)
\def\Xu{\XVec{u}}              % unknown vector
\def\Xf{\XVec{f}}              % right-hand-side vector
\def\Xe{\XVec{e}}
\def\Xr{\XVec{r}}
\def\Xx{\XVec{x}}
\def\Xv{\XVec{v}}
\def\Xw{\XVec{w}}
\def\Xn{\XVec{n}}              % normal vector
\def\Xes{\Xe_s}
\def\Xec{\Xe_c}
\def\Xus{\Xu_s}
\def\Xuc{\Xu_c}
\def\Xvs{\Xv_s}
\def\Xvc{\Xv_c}
\def\bone{{\boldsymbol 1}}
\def\bphi{{\boldsymbol \varphi}}
\def\bpsi{{\boldsymbol \psi}}
\def\btheta{{\boldsymbol \theta}}
\def\bchi{{\boldsymbol \chi}}
\def\boldeta{{\boldsymbol \eta}}
\def\bolddelta{{\boldsymbol \delta}}
\def\bsigma{{\boldsymbol \sigma}}
\def\btau{{\boldsymbol \tau }}
\def\bdelta{{\boldsymbol \delta}}
 
\def\bell{{\boldsymbol \ell}}
\def\Nedelec{N\'ed\'elec\ }



\def\bPi{{\boldsymbol \Pi}}
\def\bPhi{{\boldsymbol \Phi}}

\newcommand{\ott}[1]{\bar{#1}}
 
\title{Adaptive Algebraic Multigrid with Graph Modularity Matching}

\begin{document}
\maketitle

\section{Introduction}
This library implements some tools to explore techniques of adaptive AMG.
In classical AMG, the performance of the solver depends on the construction
of the relaxation and coarse-grid correction process (cite M. BREZINA).
Convergence of the method slows when the residual approaches the rhs of the
system (the iterate is in the `near null space` of the system).
This happens when the current iterate is an approximation of a minimal
eigenvalue of the matrix. Using this near null component and utilizing graph
modularity, we can construct a new relaxation and coarse-grid correction.
This provides the adaptivity when convergence suffers. Combining multiple of
these adaptive relaxation steps can provide a composite adaptive multigrid method.

\section{Library Details}
The library is broken into 3 main modules: partitioner, preconditioner, and solver.

\subsection{Partitioner module}
Here is implemented an algorithm described in (quiring 2019) which is graph pairwise
matching algorithm that only merges vertices where the change in modularity is
positive. The only requirement for this algorithm is that the matrix has positive
row sums. To convert our system to have positive rowsums, we construct a new system
with a matching sparsity pattern using a `near null` component. The modularity
matching algorithm applied to this new system then gives a hierarchy of interpolation
matrices that can be applied the the original system to create the coarse problem.

\subsection{Preconditioner module}
This module is called preconditioner but maybe a better name could be smoothers or
methods. In this module there are functions that take a system and provide the inverse
action of some method than can be used in an iterative solver. Currently, one can build 
L1 smoother, symmetric/forward/backward Gauss-Seidel, multilevel L1, and multilevel
Gauss-Seidel. All of these can be used as methods for the stationary solver but only
the symmetric methods can be used as a preconditioner in the conjugate gradient method.

\subsection{Solver module}
This module implements different iterative solvers that accept linear systems, a
method to base the solver on, and some parameters for the solver. Currently, only
the classical stationary iterative method and preconditioned conjugate gradient is
implemented. Soon I will be adding an adaptive solver to this module.

\section{Example Usage}
The library is currently packaged with a CLI binary that shows how the library
can be used to test different solvers on your systems.

\section{Goals}
Classical AMG often breaks down on highly anisotropic problems often found in finite
element methods. To address this failure, sometimes specific knowledge of the system
can be used to carefully construct a preconidtioner. The goal of this research is to
test some adaptive methods that don't require any knowledge about the system and
measure the computational complexity and memory demands of these methods on systems
where classical AMG performs poorly.

\section{Algorithm}
Let $A = (a_{ij})^n_{i,j=1}$ be an $n \times n$  s.p.d. sparse matrix. We begin by searching
for an algebraically smooth error vector, $\bw = (w_j)  \in {\mathbb R}^n$,
such that $A \bw \approx 0$. To obtain $\bw$:
\begin{itemize}
   \item Begin with a random starting vector and some smoother, either L1 or Gauss-Seidel,
      and perform a few iterations of relaxation. 
   \item Construct the weighted matrix ${\overline A} = ({\overline a}_{ij})^n_{i,j=1}$ where
      ${\overline a}_{ij} = - w_i a_{ij}w_j$ if $i \ne j$ and ${\overline a}_{ii} = 0.$
      Note that its row sums are positive if $\bw$ is a good enough approximation to the 
      homogeneous system,
      \begin{equation*}
         r_i \equiv \sum\limits_j{\overline a}_{ij} \approx  a_{ii} w^2_i \ge 0 \text{ since } a_{ii} >0.
      \end{equation*}
      If the rowsums $(r_i)$ are not all positive, go back to smoothing but starting with $\bw$ instead
      of a new random vector for a few more iterations. 
\end{itemize}

Once we have a suitable $\bw$ that produces an ${\overline A}$ with positive rowsums, 
we now consider the modularity matrix $B = (b_{ij})$ associated with ${\overline A}$:
Let $\br = (r_i)^n_{i=1}$ and $T = \sum\limits^n_{i=1} r_i$. 
\begin{equation*}
B = {\overline A} - \frac{1}{T} \br \br^T.
\end{equation*}
That is, the entries of $B$ are $b_{ij} = {\overline a}_{ij} -\frac{1}{T}r_ir_j$.
We do not form $B$ explicitly (since it will destroy the sparsity), i.e., we keep its sparse
component ${\overline A}$ and the vector $\br$ separately. 

Using $B$, we construct a hierarchy of interpolation matrices $[P_k]_{k=0}^l$ which define
the hierarchy of coarse grid matrices used for AMG. $A_0 = A$ and $A_{k+1} = P^T_k A_k P^k$.
To obtain one level of coarsening:
\begin{itemize}
   \item Apply Luby's matching algorithm to $B$ by merging locally maximal edges, only considering
      positive values. This gives an intermediate pairwise interpolation matrix, $P$.
   \item Obtain $B_c$ by applying the interpolation to ${\overline A}$ and $\br$. Go back to step
      one to apply Luby's to $B_c$. After each recursive step, accumulate the new interpolation
      matrix into the old one by $P := PP_{new}$. Stop once $P$ provides the desired coarsening
      factor and append it to the hierarchy.
\end{itemize}
Repeat the above steps until $B$ no longer has any positive entries or the coarsest problem is small
enough to solve directly.

This hierarchy of coarse problems with a chosen smoother defines a V-cycle AMG operator $B_V$ and
its inverse.

Adding $B_V$ to a sequence of AMG operators defines a composite method. This composite method can
now be used in place of the original smoother to search for another smooth error $\bw$ vector to
repeat all of the steps of the algorithm above. This allows the adaptivity of the solver. After
each time adding a new multilevel $B_V$ to the composite operator, we can test its convergence
rate on the homogeneous problem with a random starting vector. We stop adding components when
the convergence rate is satisfactory.
\end{document}
